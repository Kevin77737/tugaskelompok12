{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uyIfY1qlPt1"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3oaaBhxGiZvY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "\n",
        "# Transformer modules\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttXqV35mlW5R"
      },
      "source": [
        "# Import File, Extract, & Ubah CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lvig9H1_S2q"
      },
      "source": [
        "Import File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4MZP0IE_Ecd",
        "outputId": "be2b14b2-a3b6-4c5e-f3e5-4fddcc730abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File di folder setelah ekstraksi:\n",
            "['Full_Trace_Attack_Data', 'Full_Trace_Validation_Data', '.DS_Store', 'Full_Trace_Training_Data']\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path file ZIP dan folder tujuan ekstraksi\n",
        "zip_path = '/content/Full_Process_Traces_2.zip'\n",
        "extract_path = '/content/Full_Process_Traces'  # Ganti path agar lebih sederhana\n",
        "\n",
        "# Ekstrak file ZIP\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Periksa isi folder setelah diekstraksi\n",
        "print(\"File di folder setelah ekstraksi:\")\n",
        "print(os.listdir(extract_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BoiYpWv_W5v"
      },
      "source": [
        "Mengubah dataset ke CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrpcMZBe-DVU"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ihm0-B7XEOtS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Path folder yang berisi file log\n",
        "log_dir = '/content/Full_Process_Traces'\n",
        "\n",
        "# Inisialisasi list untuk menyimpan data dan label\n",
        "all_events = []\n",
        "all_labels = []\n",
        "\n",
        "# Loop melalui semua file di folder log_dir\n",
        "for filename in os.listdir(log_dir):\n",
        "    file_path = os.path.join(log_dir, filename)\n",
        "\n",
        "    # Pastikan file adalah file teks (log file)\n",
        "    if os.path.isfile(file_path) and filename.endswith(\".log\"):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                # Baca semua baris dalam file log\n",
        "                lines = f.readlines()\n",
        "\n",
        "                # Ekstrak event dan label dari setiap baris\n",
        "                # Asumsi format: <event> <label>\n",
        "                # Anda perlu menyesuaikan ini dengan format data Anda\n",
        "                for line in lines:\n",
        "                    parts = line.strip().split(' ')\n",
        "                    event = ' '.join(parts[:-1])  # Gabungkan semua bagian kecuali label\n",
        "                    label = parts[-1]  # Bagian terakhir adalah label\n",
        "\n",
        "                    all_events.append(event)\n",
        "                    all_labels.append(label)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saat memproses '{file_path}': {e}\")\n",
        "\n",
        "# Encode label (jika perlu)\n",
        "label_encoder = LabelEncoder()\n",
        "all_labels = label_encoder.fit_transform(all_labels)\n",
        "\n",
        "# Tokenisasi dan Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_events)\n",
        "# Change 'tokenizer.texts_to' to 'tokenizer.texts_to_sequences' to call the correct method\n",
        "sequences = tokenizer.texts_to_sequences(all_events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLofwxsj-H0t"
      },
      "source": [
        "# Membangun Model TB-Detector Berbasis Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import warnings\n",
        "import zipfile\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import subprocess\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for model parameters\"\"\"\n",
        "    MAX_LENGTH = 512\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 100\n",
        "    LEARNING_RATE = 2e-5\n",
        "    TRAIN_SIZE = 0.8\n",
        "    VAL_SIZE = 0.2\n",
        "    RANDOM_STATE = 42\n",
        "    NUM_LABELS = 2\n",
        "    TBDETECTOR_REPO = \"https://github.com/dazhi-ui/TBDetector.git\"\n",
        "    TBDETECTOR_PATH = \"TBDetector\"\n",
        "\n",
        "\n",
        "def clone_tbdetector():\n",
        "    \"\"\"Clone TBDetector repository if not exists\"\"\"\n",
        "    if not os.path.exists(Config.TBDETECTOR_PATH):\n",
        "        subprocess.run([\"git\", \"clone\", Config.TBDETECTOR_REPO])\n",
        "        print(f\"TBDetector repository cloned to {Config.TBDETECTOR_PATH}\")\n",
        "    else:\n",
        "        print(f\"TBDetector repository already exists in {Config.TBDETECTOR_PATH}\")\n",
        "\n",
        "\n",
        "class TBDetectorModel(nn.Module):\n",
        "    \"\"\"TBDetector model integrating with the GitHub repository\"\"\"\n",
        "    def __init__(self):\n",
        "        super(TBDetectorModel, self).__init__()\n",
        "        # Ensure TBDetector is cloned\n",
        "        clone_tbdetector()\n",
        "        # Base BERT model\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=Config.NUM_LABELS)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# ... (rest of your code from the previous response) ...\n",
        "\n",
        "# Tokenisasi dan Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_events)\n",
        "sequences = tokenizer.texts_to_sequences(all_events) # Corrected method call\n",
        "padded_sequences = pad_sequences(sequences, maxlen=Config.MAX_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "metadata": {
        "id": "q1SVQ2kXKpty"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcUq4HAu-av5"
      },
      "source": [
        "# Menggunakan TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "import numpy as np # Import numpy for debugging\n",
        "\n",
        "# Setup directory untuk log TensorBoard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "# Callback lainnya (misalnya EarlyStopping)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "qvvjDCupK2XH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGZGFHEE-eSt"
      },
      "source": [
        "# Melatih Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_events)\n",
        "sequences = tokenizer.texts_to_sequences(all_events)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=Config.MAX_LENGTH, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "1kkrpa3hK8ez"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dummy Config class for parameters\n",
        "class Config:\n",
        "    TRAIN_SIZE = 0.8\n",
        "    RANDOM_STATE = 42\n",
        "    LEARNING_RATE = 0.001\n",
        "    EPOCHS = 10\n",
        "    PRINT_INTERVAL = 1\n",
        "\n",
        "# Dummy model definition\n",
        "class TBDetectorModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TBDetectorModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, 50)  # Assuming padded_sequences shape (1000, 100)\n",
        "        self.fc2 = nn.Linear(50, 10)    # Assuming 10 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Example padded_sequences and all_labels\n",
        "padded_sequences = np.random.rand(1000, 100)  # Simulated input\n",
        "all_labels = np.random.randint(0, 10, size=(1000,))  # Simulated labels\n",
        "\n",
        "# Print shapes for debugging\n",
        "print(\"Shape of padded_sequences:\", padded_sequences.shape)\n",
        "print(\"Shape of all_labels:\", np.array(all_labels).shape)\n",
        "\n",
        "# Check if padded_sequences is empty\n",
        "if padded_sequences.size == 0:\n",
        "    print(\"Error: padded_sequences is empty. Check your data preprocessing.\")\n",
        "    exit()  # Stop execution to prevent the error\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    padded_sequences, all_labels, train_size=Config.TRAIN_SIZE, random_state=Config.RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Convert data to PyTorch tensors with correct dtype\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # Change to float32\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)      # Change to float32\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "# Create the TBDetector model\n",
        "model = TBDetectorModel()\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "X_train_tensor = X_train_tensor.to(device)\n",
        "y_train_tensor = y_train_tensor.to(device)\n",
        "X_val_tensor = X_val_tensor.to(device)\n",
        "y_val_tensor = y_val_tensor.to(device)\n",
        "\n",
        "num_epochs = Config.EPOCHS\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)  # Forward pass through the model\n",
        "    loss = loss_fn(outputs, y_train_tensor)  # Calculate loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update model parameters\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % Config.PRINT_INTERVAL == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        val_outputs = model(X_val_tensor)  # Forward pass on validation data\n",
        "        val_loss = loss_fn(val_outputs, y_val_tensor)  # Calculate validation loss\n",
        "        _, predicted = torch.max(val_outputs, 1)  # Get the predicted class\n",
        "        correct = (predicted == y_val_tensor).sum().item()  # Count correct predictions\n",
        "        val_accuracy = correct / y_val_tensor.size(0)  # Calculate validation accuracy\n",
        "\n",
        "        # Print validation metrics\n",
        "        print(f'Validation Loss: {val_loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB6nk3tAOXeY",
        "outputId": "62515bfa-17ca-4107-a425-20cfb5a79423"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded_sequences: (1000, 100)\n",
            "Shape of all_labels: (1000,)\n",
            "Epoch [1/10], Loss: 2.3058\n",
            "Validation Loss: 2.3107, Validation Accuracy: 0.1000\n",
            "Epoch [2/10], Loss: 2.3001\n",
            "Validation Loss: 2.3120, Validation Accuracy: 0.0950\n",
            "Epoch [3/10], Loss: 2.2960\n",
            "Validation Loss: 2.3138, Validation Accuracy: 0.0700\n",
            "Epoch [4/10], Loss: 2.2927\n",
            "Validation Loss: 2.3158, Validation Accuracy: 0.0800\n",
            "Epoch [5/10], Loss: 2.2901\n",
            "Validation Loss: 2.3177, Validation Accuracy: 0.0900\n",
            "Epoch [6/10], Loss: 2.2879\n",
            "Validation Loss: 2.3196, Validation Accuracy: 0.0900\n",
            "Epoch [7/10], Loss: 2.2859\n",
            "Validation Loss: 2.3214, Validation Accuracy: 0.0950\n",
            "Epoch [8/10], Loss: 2.2839\n",
            "Validation Loss: 2.3228, Validation Accuracy: 0.1050\n",
            "Epoch [9/10], Loss: 2.2820\n",
            "Validation Loss: 2.3239, Validation Accuracy: 0.1000\n",
            "Epoch [10/10], Loss: 2.2799\n",
            "Validation Loss: 2.3247, Validation Accuracy: 0.1150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQyU7nn5-oOi"
      },
      "source": [
        "# Menjalankan TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ce0r_qf_9k81"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Your model, training data, and configurations here\n",
        "# ...\n",
        "\n",
        "# Set up TensorBoard writer\n",
        "writer = SummaryWriter(log_dir='logs/fit')\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass and loss calculation\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = loss_fn(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log metrics to TensorBoard\n",
        "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
        "    # Optionally log other metrics such as accuracy here\n",
        "\n",
        "# Close the writer after training\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45dRyE_V-phA"
      },
      "source": [
        "# Evaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8TssypCi9nYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a8ad07-39a4-4252-b56f-83454f89809e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 10.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Your model, training data, and configurations here\n",
        "# ...\n",
        "# Split data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    padded_sequences, all_labels, train_size=Config.TRAIN_SIZE, random_state=Config.RANDOM_STATE\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, train_size=0.5, random_state=Config.RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Convert test data to PyTorch tensors and move to device\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "def evaluate(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the test data and returns the loss and accuracy.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to evaluate.\n",
        "        X_test (torch.Tensor): The test data.\n",
        "        y_test (torch.Tensor): The test labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the loss and accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        outputs = model(X_test)  # Forward pass on test data\n",
        "        loss = loss_fn(outputs, y_test)  # Calculate loss\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
        "        correct = (predicted == y_test).sum().item()  # Count correct predictions\n",
        "        accuracy = correct / y_test.size(0)  # Calculate accuracy\n",
        "\n",
        "    return loss.item(), accuracy  # Return loss and accuracy\n",
        "\n",
        "# ... (Rest of your code remains the same)\n",
        "\n",
        "# Assuming you have X_test and y_test loaded as PyTorch tensors\n",
        "# and moved to the correct device (CPU or GPU)\n",
        "loss, accuracy = evaluate(model, X_test, y_test)  # Call the evaluate function\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}