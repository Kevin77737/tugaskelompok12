{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kevin77737/tugaskelompok12/blob/main/1301213050_LTRdetector_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RIK51l2G3zZ"
      },
      "source": [
        "#### 1. Import"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modul dan dataset (download dengan wget)"
      ],
      "metadata": {
        "id": "ikB6aZnq_zkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Keq7_3oVW_5s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aUUNmPW7ADNf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sqlite3\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ambil data dari web dengan menggunakan 'wget'"
      ],
      "metadata": {
        "id": "ZI8JEjeRJgWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BnXkP2WJxLH",
        "outputId": "cb5aeb5c-ed73-4e6f-b81d-9f77d5f4707a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-31 03:38:36--  https://dataverse.harvard.edu/api/access/datafile/8083475\n",
            "Resolving dataverse.harvard.edu (dataverse.harvard.edu)... 3.228.243.207, 3.216.74.121, 107.23.189.154\n",
            "Connecting to dataverse.harvard.edu (dataverse.harvard.edu)|3.228.243.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://dvn-cloud.s3.amazonaws.com/10.7910/DVN/IFTZPF/18cd2447b7a-2f625bb2a449?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27Full_Process_Traces%25202.zip&response-content-type=application%2Fzip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241031T033838Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=AKIAIEJ3NV7UYCSRJC7A%2F20241031%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=969719d42590a16dc3e6be36fdeefe82768b36ef6db9cef4598aab3e6709712f [following]\n",
            "--2024-10-31 03:38:38--  https://dvn-cloud.s3.amazonaws.com/10.7910/DVN/IFTZPF/18cd2447b7a-2f625bb2a449?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27Full_Process_Traces%25202.zip&response-content-type=application%2Fzip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241031T033838Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=AKIAIEJ3NV7UYCSRJC7A%2F20241031%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=969719d42590a16dc3e6be36fdeefe82768b36ef6db9cef4598aab3e6709712f\n",
            "Resolving dvn-cloud.s3.amazonaws.com (dvn-cloud.s3.amazonaws.com)... 3.5.30.83, 52.217.232.217, 52.216.209.137, ...\n",
            "Connecting to dvn-cloud.s3.amazonaws.com (dvn-cloud.s3.amazonaws.com)|3.5.30.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31053543 (30M) [application/zip]\n",
            "Saving to: ‘Full_Process_Traces_2.zip’\n",
            "\n",
            "Full_Process_Traces 100%[===================>]  29.61M  8.46MB/s    in 3.5s    \n",
            "\n",
            "2024-10-31 03:38:42 (8.46 MB/s) - ‘Full_Process_Traces_2.zip’ saved [31053543/31053543]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Download datasets dari 'https://dataverse.harvard.edu/file.xhtml?fileId=8083475',\n",
        "lalu save di storage session (Download lagi ketika memulai sesi baru)\n",
        "/content/Full_Process_Traces_2\n",
        "'''\n",
        "!wget https://dataverse.harvard.edu/api/access/datafile/8083475 -O 'Full_Process_Traces_2.zip'\n",
        "!unzip -q 'Full_Process_Traces_2.zip' -d /content/'Full_Process_Traces_2'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy4BxC9KG_0S"
      },
      "source": [
        "#### 2. Membaca Dataset (Preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_adfa_ids(dataset_dir):\n",
        "    data = []\n",
        "    for root, dirs, files in os.walk(dataset_dir):\n",
        "        for file in files:\n",
        "\n",
        "            try:\n",
        "                with open(os.path.join(root, file), 'r', encoding='latin-1') as f:\n",
        "                    sequence = f.readlines()\n",
        "                    sequence = [line.strip() for line in sequence]\n",
        "                    data.append(sequence)\n",
        "            except UnicodeDecodeError:\n",
        "\n",
        "                try:\n",
        "                    with open(os.path.join(root, file), 'r', encoding='utf-16') as f:\n",
        "                        sequence = f.readlines()\n",
        "                        sequence = [line.strip() for line in sequence]\n",
        "                        data.append(sequence)\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"Error reading file: {file}. Skipping...\")\n",
        "    return [' '.join(seq) for seq in data]"
      ],
      "metadata": {
        "id": "4zeuElf0IYRq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_data = load_adfa_ids('/content/Full_Process_Traces_2/Full_Process_Traces/Full_Trace_Attack_Data')\n",
        "normal_data = load_adfa_ids('/content/Full_Process_Traces_2/Full_Process_Traces/Full_Trace_Training_Data')\n",
        "\n",
        "combined_data = normal_data + attack_data"
      ],
      "metadata": {
        "id": "NYHfxh5mI0cQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with sequences only\n",
        "df = pd.DataFrame({\n",
        "    'sequence': combined_data\n",
        "})\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DapGIfIKKq8g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(5))\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksB7iLclKz05",
        "outputId": "1e742347-4fcf-46ee-c508-050ccfd01c82"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sequence\n",
            "0                                kernel32.dll+0xb511\n",
            "1           kernel32.dll+0x1084d kernel32.dll+0xb50b\n",
            "2  kernel32.dll+0xb50b kernel32.dll+0xb50b kernel...\n",
            "3  kernel32.dll+0xc939 ntdll.dll+0x10b63 kernel32...\n",
            "4  kernel32.dll+0x1084d ntdll.dll+0xeae3 kernel32...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5899 entries, 0 to 5898\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sequence  5899 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 46.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels (0 for normal, 1 for attack)\n",
        "data = {\n",
        "    'sequence': [['call1', 'call2', 'call3'], ['call4', 'call5']],\n",
        "    'label': [0, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the DataFrame to verify\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn_QLHPL3oUi",
        "outputId": "d0db0c3c-447a-45af-c781-eeee413ee4a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                sequence  label\n",
            "0  [call1, call2, call3]      0\n",
            "1         [call4, call5]      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Preprocessing"
      ],
      "metadata": {
        "id": "n_GZnlvXLJ-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenisasi (menggunakan Word2Vec)"
      ],
      "metadata": {
        "id": "mAsgIu_0brEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument"
      ],
      "metadata": {
        "id": "2uZxywrV62jb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = [sequence.split() for sequence in combined_data]\n",
        "\n",
        "# Tag data untuk Doc2Vec\n",
        "tagged_data = [TaggedDocument(words=seq, tags=[i]) for i, seq in enumerate(tokenized_data)]\n",
        "\n",
        "# Menggunakan Doc2Vec dengan dimensi yang lebih kecil\n",
        "model_d2v = Doc2Vec(tagged_data, vector_size=64, window=3, min_count=1, epochs=5)\n",
        "\n",
        "# Konversi setiap urutan ke vektor\n",
        "vectorized_data = [model_d2v.infer_vector(seq) for seq in tokenized_data]"
      ],
      "metadata": {
        "id": "Ot8MKpROmMvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_data = [sequence.split() for sequence in combined_data]\n",
        "\n",
        "# model_w2v = Word2Vec(sentences=tokenized_data, vector_size=64, window=3, min_count=1, sg=1, epochs=5)\n",
        "# # model_ft = FastText(sentences=tokenized_data, vector_size=64, window=3, min_count=1, sg=1, epochs=5)\n",
        "\n",
        "\n",
        "# # Menyimpan model Word2Vec untuk digunakan kembali\n",
        "# model_ft.save(\"fastText_model.bin\")\n",
        "\n",
        "# # Fungsi konversi\n",
        "# def sequence_to_vector(sequence, model):\n",
        "#     return np.mean([model.wv[word] for word in sequence if word in model.wv], axis=0)\n",
        "\n",
        "# # Conversion\n",
        "# vectorized_data = [sequence_to_vector(seq, model_ft) for seq in combined_data]\n",
        "\n",
        "# # print(f\"{len(vectorized_data)}\")\n",
        "# # print(f\" {vectorized_data[0]}\")"
      ],
      "metadata": {
        "id": "64zVasZs4lHC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. LTRDetector Transformer"
      ],
      "metadata": {
        "id": "SCip_Ns1L28g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modul yang diperlukan untuk membangun transformer disini"
      ],
      "metadata": {
        "id": "uoGJhNpd8Wnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "hgKyzMPk8WOt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fungsi encoder"
      ],
      "metadata": {
        "id": "WJa8eI7T8Ls-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Layer Multi-Head Attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
        "    attention_output = Dropout(dropout)(attention_output)\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
        "\n",
        "    # Layer Feed Forward Network\n",
        "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    return LayerNormalization(epsilon=1e-6)(ffn_output + attention_output)"
      ],
      "metadata": {
        "id": "tX9cLovF8MSd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Menyiapkan data (Preprocessing Data lanjutan)"
      ],
      "metadata": {
        "id": "x5m2da9JUfYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fungsi ini untuk mengembalikan list dari vektor embedding tiap kata dalam urutan\n",
        "def sequence_to_vector(sequence, model):\n",
        "    return [model.wv[word] for word in sequence if word in model.wv]\n",
        "\n",
        "vectorized_data = [sequence_to_vector(seq, model_w2v) for seq in tokenized_data]"
      ],
      "metadata": {
        "id": "ZISVCJ3olCJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence(seq, sequence_length, embedding_dim):\n",
        "    if len(seq) < sequence_length:\n",
        "        # Jika urutan lebih pendek, tambahkan padding (vektor nol)\n",
        "        padding = [np.zeros(embedding_dim) for _ in range(sequence_length - len(seq))]\n",
        "        seq.extend(padding)\n",
        "    else:\n",
        "        # Jika urutan lebih panjang, potong urutan\n",
        "        seq = seq[:sequence_length]\n",
        "    return np.array(seq)"
      ],
      "metadata": {
        "id": "P8N7hpk8lX2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Terapkan padding\n",
        "sequence_length = 10\n",
        "embedding_dim = 128\n",
        "vectorized_data = np.array([pad_sequence(seq, sequence_length, embedding_dim) for seq in vectorized_data])\n",
        "\n",
        "# Verifikasi bentuk data akhir\n",
        "print(f\"Shape of vectorized data: {vectorized_data.shape}\")"
      ],
      "metadata": {
        "id": "ouOwHYo0MGNo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "c6e0d845-1c7b-4222-addd-475266ef53e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'numpy.float64' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6e301d798729>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Padding atau pemotongan agar panjang setiap urutan menjadi sama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorized_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mvectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-6e301d798729>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Padding atau pemotongan agar panjang setiap urutan menjadi sama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorized_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mvectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Membangun model transformer"
      ],
      "metadata": {
        "id": "RFLhMJr88oLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter\n",
        "head_size = 64\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "num_layers = 3\n",
        "dropout = 0.2\n",
        "\n",
        "# Membangun model Transformer Encoder\n",
        "transformer_model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout)\n",
        "\n",
        "# Compiling model\n",
        "transformer_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Melatih model hanya pada data normal\n",
        "normal_data = vectorized_data[:len(normal_data)]  # Asumsi data normal ada di bagian awal dari vectorized_data\n",
        "history = transformer_model.fit(normal_data, normal_data, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Simpan model\n",
        "transformer_model.save(\"transformer_encoder_model.h5\")"
      ],
      "metadata": {
        "id": "oKYq4L0B8oey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ekstraksi fitur"
      ],
      "metadata": {
        "id": "OmMdy9WFAr9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data, model):\n",
        "    return model.predict(data)\n",
        "\n",
        "normal_features = extract_features(normal_data, transformer_model)\n",
        "# attack_features = extract_features(attack_data, transformer_model)\n",
        "\n",
        "normal_features.shape"
      ],
      "metadata": {
        "id": "nI3js1rCMFb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Evaluasi & Hasil"
      ],
      "metadata": {
        "id": "TEEpRtMNL9II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min"
      ],
      "metadata": {
        "id": "mgb7jzZ7Accg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Deteksi anomali"
      ],
      "metadata": {
        "id": "TkdAC29VAPmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomaly(new_data_features, cluster_centers, threshold):\n",
        "    # Menemukan klaster terdekat untuk setiap data baru\n",
        "    _, distances = pairwise_distances_argmin_min(new_data_features, cluster_centers)\n",
        "    # Jika jarak melebihi threshold, data dianggap anomali\n",
        "    anomalies = distances > threshold\n",
        "    return anomalies, distances"
      ],
      "metadata": {
        "id": "mSpRbwM-AzLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tentukan jumlah klaster berdasarkan eksperimen atau domain knowledge\n",
        "num_clusters = 5\n",
        "\n",
        "# Melatih K-Means pada fitur data normal\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "kmeans.fit(normal_features)\n",
        "\n",
        "# Menyimpan pusat klaster sebagai referensi\n",
        "cluster_centers = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "SSCT6G2GByAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jarak maksimum dari setiap data normal ke pusat cluster terdekat\n",
        "_, distances = pairwise_distances_argmin_min(normal_features, cluster_centers)\n",
        "threshold = np.max(distances)\n",
        "\n",
        "print(f\"Ambang batas jarak: {threshold}\")"
      ],
      "metadata": {
        "id": "UoQEHKsTA4L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah data attack atau test menjadi fitur\n",
        "attack_data_features = extract_features(vectorized_data[len(normal_data):], transformer_model)\n",
        "\n",
        "# Deteksi anomali\n",
        "anomalies, distances = detect_anomaly(attack_data_features, cluster_centers, threshold)\n",
        "\n",
        "# Output hasil deteksi\n",
        "print(\"Hasil Deteksi Anomali:\")\n",
        "for i, (is_anomaly, distance) in enumerate(zip(anomalies, distances)):\n",
        "    status = \"Anomali\" if is_anomaly else \"Normal\"\n",
        "    print(f\"Data {i+1}: {status} (jarak: {distance:.2f})\")"
      ],
      "metadata": {
        "id": "_fChzt_YBFEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi"
      ],
      "metadata": {
        "id": "OGb6cqM7AUis"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nESbuF9CERi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODPzJb8zXkConm4H8nyJTO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}