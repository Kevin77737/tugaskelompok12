{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kevin77737/tugaskelompok12/blob/main/1301213050_LTRdetector_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RIK51l2G3zZ"
      },
      "source": [
        "#### 1. Import"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modul dan dataset (download dengan wget)"
      ],
      "metadata": {
        "id": "ikB6aZnq_zkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Keq7_3oVW_5s",
        "outputId": "217ce428-ba6f-453b-aed6-79524deb7278"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aUUNmPW7ADNf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sqlite3\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ambil data dari web dengan menggunakan 'wget'"
      ],
      "metadata": {
        "id": "ZI8JEjeRJgWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BnXkP2WJxLH",
        "outputId": "56b9f4e8-0874-40b3-c02b-d9588a93acfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-30 19:22:14--  https://dataverse.harvard.edu/api/access/datafile/8083475\n",
            "Resolving dataverse.harvard.edu (dataverse.harvard.edu)... 3.228.243.207, 3.216.74.121, 107.23.189.154\n",
            "Connecting to dataverse.harvard.edu (dataverse.harvard.edu)|3.228.243.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://dvn-cloud.s3.amazonaws.com/10.7910/DVN/IFTZPF/18cd2447b7a-2f625bb2a449?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27Full_Process_Traces%25202.zip&response-content-type=application%2Fzip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241030T192214Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=AKIAIEJ3NV7UYCSRJC7A%2F20241030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=e31736a1ad968d0a13256f628b2a3286ef62e2019404ccc9766741ed0593fe47 [following]\n",
            "--2024-10-30 19:22:14--  https://dvn-cloud.s3.amazonaws.com/10.7910/DVN/IFTZPF/18cd2447b7a-2f625bb2a449?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27Full_Process_Traces%25202.zip&response-content-type=application%2Fzip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241030T192214Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=AKIAIEJ3NV7UYCSRJC7A%2F20241030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=e31736a1ad968d0a13256f628b2a3286ef62e2019404ccc9766741ed0593fe47\n",
            "Resolving dvn-cloud.s3.amazonaws.com (dvn-cloud.s3.amazonaws.com)... 3.5.29.219, 52.216.43.105, 52.217.120.113, ...\n",
            "Connecting to dvn-cloud.s3.amazonaws.com (dvn-cloud.s3.amazonaws.com)|3.5.29.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31053543 (30M) [application/zip]\n",
            "Saving to: ‘Full_Process_Traces_2.zip’\n",
            "\n",
            "Full_Process_Traces 100%[===================>]  29.61M  13.4MB/s    in 2.2s    \n",
            "\n",
            "2024-10-30 19:22:17 (13.4 MB/s) - ‘Full_Process_Traces_2.zip’ saved [31053543/31053543]\n",
            "\n",
            "replace /content/Full_Process_Traces_2/Full_Process_Traces/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/Full_Process_Traces_2/__MACOSX/Full_Process_Traces/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n",
            "A\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Download datasets dari 'https://dataverse.harvard.edu/file.xhtml?fileId=8083475',\n",
        "lalu save di storage session (Download lagi ketika memulai sesi baru)\n",
        "/content/Full_Process_Traces_2\n",
        "'''\n",
        "!wget https://dataverse.harvard.edu/api/access/datafile/8083475 -O 'Full_Process_Traces_2.zip'\n",
        "!unzip -q 'Full_Process_Traces_2.zip' -d /content/'Full_Process_Traces_2'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy4BxC9KG_0S"
      },
      "source": [
        "#### 2. Membaca Dataset (Preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_adfa_ids(dataset_dir):\n",
        "    data = []\n",
        "    for root, dirs, files in os.walk(dataset_dir):\n",
        "        for file in files:\n",
        "\n",
        "            try:\n",
        "                with open(os.path.join(root, file), 'r', encoding='latin-1') as f:\n",
        "                    sequence = f.readlines()\n",
        "                    sequence = [line.strip() for line in sequence]\n",
        "                    data.append(sequence)\n",
        "            except UnicodeDecodeError:\n",
        "\n",
        "                try:\n",
        "                    with open(os.path.join(root, file), 'r', encoding='utf-16') as f:\n",
        "                        sequence = f.readlines()\n",
        "                        sequence = [line.strip() for line in sequence]\n",
        "                        data.append(sequence)\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"Error reading file: {file}. Skipping...\")\n",
        "    return [' '.join(seq) for seq in data]"
      ],
      "metadata": {
        "id": "4zeuElf0IYRq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_data = load_adfa_ids('/content/Full_Process_Traces_2/Full_Process_Traces/Full_Trace_Attack_Data')\n",
        "normal_data = load_adfa_ids('/content/Full_Process_Traces_2/Full_Process_Traces/Full_Trace_Training_Data')\n",
        "\n",
        "combined_data = normal_data + attack_data"
      ],
      "metadata": {
        "id": "NYHfxh5mI0cQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with sequences only\n",
        "df = pd.DataFrame({\n",
        "    'sequence': combined_data\n",
        "})\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DapGIfIKKq8g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(5))\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksB7iLclKz05",
        "outputId": "6980a92f-6bdb-4abc-8915-71d639b36ad2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sequence\n",
            "0  kernel32.dll+0x2ec17 kernel32.dll+0x2f02b kern...\n",
            "1  kernel32.dll+0x2ec17 kernel32.dll+0x2f02b kern...\n",
            "2  kernel32.dll+0xb50b kernel32.dll+0xb50b kernel...\n",
            "3  kernel32.dll+0x1084d kernel32.dll+0x1084d kern...\n",
            "4  kernel32.dll+0xb50b kernel32.dll+0x23eab kerne...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5899 entries, 0 to 5898\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sequence  5899 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 46.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels (0 for normal, 1 for attack)\n",
        "data = {\n",
        "    'sequence': [['call1', 'call2', 'call3'], ['call4', 'call5']],\n",
        "    'label': [0, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the DataFrame to verify\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn_QLHPL3oUi",
        "outputId": "23ad74fc-2b8e-425f-e542-b0f8123ba0ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                sequence  label\n",
            "0  [call1, call2, call3]      0\n",
            "1         [call4, call5]      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Preprocessing"
      ],
      "metadata": {
        "id": "n_GZnlvXLJ-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenisasi (menggunakan Word2Vec)"
      ],
      "metadata": {
        "id": "mAsgIu_0brEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "2uZxywrV62jb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = [sequence.split() for sequence in combined_data]\n",
        "\n",
        "model_w2v = Word2Vec(sentences=tokenized_data, vector_size=128, window=5, min_count=1, sg=1, epochs=10)\n",
        "\n",
        "# Menyimpan model Word2Vec untuk digunakan kembali\n",
        "model_w2v.save(\"word2vec_model.bin\")\n",
        "\n",
        "# Fungsi konversi\n",
        "def sequence_to_vector(sequence, model):\n",
        "    return np.mean([model.wv[word] for word in sequence if word in model.wv], axis=0)\n",
        "\n",
        "# Conversion\n",
        "vectorized_data = [sequence_to_vector(seq, model_w2v) for seq in combined_data]\n",
        "\n",
        "# print(f\"{len(vectorized_data)}\")\n",
        "# print(f\" {vectorized_data[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64zVasZs4lHC",
        "outputId": "5b835ac1-c722-4df6-a462-f442a7193456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. LTRDetector Transformer"
      ],
      "metadata": {
        "id": "SCip_Ns1L28g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modul yang diperlukan untuk membangun transformer disini"
      ],
      "metadata": {
        "id": "uoGJhNpd8Wnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "hgKyzMPk8WOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fungsi encoder"
      ],
      "metadata": {
        "id": "WJa8eI7T8Ls-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Layer Multi-Head Attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
        "    attention_output = Dropout(dropout)(attention_output)\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
        "\n",
        "    # Layer Feed Forward Network\n",
        "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    return LayerNormalization(epsilon=1e-6)(ffn_output + attention_output)"
      ],
      "metadata": {
        "id": "tX9cLovF8MSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Menyiapkan data (Preprocessing Data lanjutan)"
      ],
      "metadata": {
        "id": "x5m2da9JUfYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ubah data menjadi bentuk tensor (n_samples, sequence_length, embedding_dim)\n",
        "sequence_length = 10  # Atur sesuai kebutuhan\n",
        "embedding_dim = 128  # Harus sesuai dengan Word2Vec\n",
        "\n",
        "# Padding atau pemotongan agar panjang setiap urutan menjadi sama\n",
        "vectorized_data = [np.pad(vec, (0, max(0, sequence_length - len(vec))))[:sequence_length] for vec in vectorized_data]\n",
        "vectorized_data = np.array(vectorized_data)\n",
        "\n",
        "# Bentuk akhir dari data\n",
        "input_shape = vectorized_data.shape[1:]  # (sequence_length, embedding_dim)"
      ],
      "metadata": {
        "id": "ouOwHYo0MGNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Membangun model transformer"
      ],
      "metadata": {
        "id": "RFLhMJr88oLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter\n",
        "head_size = 64\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "num_layers = 3\n",
        "dropout = 0.2\n",
        "\n",
        "# Membangun model Transformer Encoder\n",
        "transformer_model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout)\n",
        "\n",
        "# Compiling model\n",
        "transformer_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Melatih model hanya pada data normal\n",
        "normal_data = vectorized_data[:len(normal_data)]  # Asumsi data normal ada di bagian awal dari vectorized_data\n",
        "history = transformer_model.fit(normal_data, normal_data, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Simpan model\n",
        "transformer_model.save(\"transformer_encoder_model.h5\")"
      ],
      "metadata": {
        "id": "oKYq4L0B8oey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ekstraksi fitur"
      ],
      "metadata": {
        "id": "OmMdy9WFAr9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data, model):\n",
        "    return model.predict(data)\n",
        "\n",
        "normal_features = extract_features(normal_data, transformer_model)\n",
        "# attack_features = extract_features(attack_data, transformer_model)\n",
        "\n",
        "normal_features.shape"
      ],
      "metadata": {
        "id": "nI3js1rCMFb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Evaluasi & Hasil"
      ],
      "metadata": {
        "id": "TEEpRtMNL9II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min"
      ],
      "metadata": {
        "id": "mgb7jzZ7Accg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Deteksi anomali"
      ],
      "metadata": {
        "id": "TkdAC29VAPmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomaly(new_data_features, cluster_centers, threshold):\n",
        "    # Menemukan klaster terdekat untuk setiap data baru\n",
        "    _, distances = pairwise_distances_argmin_min(new_data_features, cluster_centers)\n",
        "    # Jika jarak melebihi threshold, data dianggap anomali\n",
        "    anomalies = distances > threshold\n",
        "    return anomalies, distances"
      ],
      "metadata": {
        "id": "mSpRbwM-AzLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tentukan jumlah klaster berdasarkan eksperimen atau domain knowledge\n",
        "num_clusters = 5\n",
        "\n",
        "# Melatih K-Means pada fitur data normal\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "kmeans.fit(normal_features)\n",
        "\n",
        "# Menyimpan pusat klaster sebagai referensi\n",
        "cluster_centers = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "SSCT6G2GByAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jarak maksimum dari setiap data normal ke pusat cluster terdekat\n",
        "_, distances = pairwise_distances_argmin_min(normal_features, cluster_centers)\n",
        "threshold = np.max(distances)\n",
        "\n",
        "print(f\"Ambang batas jarak: {threshold}\")"
      ],
      "metadata": {
        "id": "UoQEHKsTA4L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah data attack atau test menjadi fitur\n",
        "attack_data_features = extract_features(vectorized_data[len(normal_data):], transformer_model)\n",
        "\n",
        "# Deteksi anomali\n",
        "anomalies, distances = detect_anomaly(attack_data_features, cluster_centers, threshold)\n",
        "\n",
        "# Output hasil deteksi\n",
        "print(\"Hasil Deteksi Anomali:\")\n",
        "for i, (is_anomaly, distance) in enumerate(zip(anomalies, distances)):\n",
        "    status = \"Anomali\" if is_anomaly else \"Normal\"\n",
        "    print(f\"Data {i+1}: {status} (jarak: {distance:.2f})\")"
      ],
      "metadata": {
        "id": "_fChzt_YBFEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi"
      ],
      "metadata": {
        "id": "OGb6cqM7AUis"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nESbuF9CERi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxjRaMY+oZUXFzaWKWjRx+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}